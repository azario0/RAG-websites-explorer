{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation de l'API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEYe = 'YOUR_API_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telechargement du HTML du site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "url = 'https://fr.wikipedia.org/wiki/Alg%C3%A9rie'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "  # Get the HTML content\n",
    "  html_content = response.text\n",
    "\n",
    "  # Save the HTML content to a file\n",
    "  with open('downloaded_page.html', 'w', encoding='utf-8') as file:\n",
    "    file.write(html_content)\n",
    "    print('HTML downloaded successfully!')\n",
    "else:\n",
    "  print(f'Error downloading the page. Status code: {response.status_code}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding et sauvegard du side dans une base de donnees vectorielle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 96\n",
      "96 192\n",
      "192 288\n",
      "288 384\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "document = UnstructuredHTMLLoader('downloaded_page.html').load()\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "documents = text_splitter.split_documents(document)\n",
    "\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model='models/text-embedding-004',google_api_key=API_KEYe)\n",
    "batch_size = 96\n",
    "num_batches = len(documents) // batch_size + (len(documents) % batch_size > 0)\n",
    "texts = [\"\", \"\"]\n",
    "db = FAISS.from_texts(texts, embeddings)\n",
    "retv = db.as_retriever()\n",
    "for batch_num in range(num_batches):\n",
    "    start_index = batch_num * batch_size\n",
    "    end_index = (batch_num + 1) * batch_size\n",
    "    batch_documents = documents[start_index:end_index]\n",
    "    retv.add_documents(batch_documents)\n",
    "    print(start_index, end_index)\n",
    "db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation du RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",google_api_key=API_KEYe)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model='models/text-embedding-004',google_api_key=API_KEYe)\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "db = FAISS.load_local(\"faiss_index\", embeddings,allow_dangerous_deserialization=True)\n",
    "retv = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 20})\n",
    "\n",
    "template = \"\"\"You are a helpful assistant. Use the following pieces of context from this html page and conversation history to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Document context: {context}\n",
    "Conversation history: {chat_history}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"chat_history\", \"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "\n",
    "# Set up memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Create the conversational retrieval chain with the custom prompt\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retv,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt_template},\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un petit test du system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the capital of Algeria?\n",
      "A: Alger is the capital of Algeria. \n",
      "\n",
      "\n",
      "Q: What's the population of the country ?\n",
      "A: The population of Algeria is 44.6 million as of January 2021. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "def ask_question(question):\n",
    "    global chat_history\n",
    "    result = qa_chain({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    return result[\"answer\"]\n",
    "\n",
    "# Example usage\n",
    "question1 = \"What is the capital of Algeria?\"\n",
    "answer1 = ask_question(question1)\n",
    "print(f\"Q: {question1}\\nA: {answer1}\\n\")\n",
    "\n",
    "question2 = \"What's its population?\"\n",
    "answer2 = ask_question(question2)\n",
    "print(f\"Q: {question2}\\nA: {answer2}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
